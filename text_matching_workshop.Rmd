---
title: "Text Analysis: Matching and Linking"
author: "Northwestern IT Research Computing Services"
date: "November 21, 2019"
output: html_document
---

### 1: Getting Started

Welcome to the Text Analysis: Matching and Linking Workshop! Today, we will discuss the use of fuzzy linkage techniques to merge data sets at the research scale (small to medium size data sets). If you have any specific questions following the workshop, please feel free to email me (*austinalleman2020\@u.northwestern.edu*) or [request a consultation with RCS](https://www.it.northwestern.edu/service-catalog/research/data-services/data-science.html).

##### 1.1: Installing and Loading Packages

The packages we depend on in this session are checked for and, if necessary, installed below.

```{r, message = FALSE}
# Check if tidyverse is installed. If not, install it. This package standardizes a wide variety of data manipulation and modeling packages.
if(!require(tidyverse)){
    install.packages("tidyverse")
}

# Check if stringdist is installed. If not, install it. This package implements string distance/similarity metrics.
if(!require(stringdist)){
    install.packages("stringdist")
}

# Check if fuzzyjoin is installed. If not, install it. This package implements joins based on string distances.
if(!require(fuzzyjoin)){
    install.packages("fuzzyjoin")
}

# Check if fuzzyjoin is installed. If not, install it. This package implements joins based on string distances.
if(!require(skimr)){
    install.packages("skimr")
}

# Load libraries.
library(tidyverse)
library(stringdist)
library(fuzzyjoin)
library(skimr)
```

##### 1.2: Preliminaries

In this workshop, we will lean on `tidyverse` data manipulation techniques. The functions you should be familiar with, and some examples on a toy data set:

```{r}
# Creating toy data set
df <- tibble(x = c(1,2,3,4,5,6), 
             y = c("R", "C", "S", "R", "C", "S"))
df
```
`select(df, vars)`: selects variables by name or using helper functions from `df`
```{r}
select(df, x)
```
`filter(df, cond)`: returns rows from `df` where `cond` is `TRUE`
```{r}
filter(df, y == "R")
```
`arrange(df, vars)`: arranges rows from `df` by ascending values of `vars`, in order (can use `desc(var)`)
```{r}
arrange(df, desc(x))
```
`mutate(df, expr)`: creates new variables from the contents of `df` according to `expr`
```{r}
mutate(df, a = 2*x, b = paste0(y, "!"))
```
`group_by(df, vars)`: specifies that operations should be done within groups defined by combinations of values of `vars`
```{r}
group_by(df, y)
```
`summarize(df, fcns)`: produces aggregates `fcns`, and when used in conjunction with `group_by`, aggregates within groups.
```{r}
summarize(df, x_sum = sum(x))
```
`View(df)`: opens `df` in the data browser for easy viewing
```{r, eval = FALSE}
View(df)
```
`%>%`: supplies the result on its left as the implicit first argument to an operation on its right
```{r, eval = FALSE}
df %>% 
  View()   # same as View(df)
```

##### 1.2.1: Exercise 

Given data frame `df` below, use `mutate`, `select`, `filter`, `group_by`, `summarize`, and `%>%` to do the following in *one continuous statement*:

* create a new column `z` which is equal to times the value of `x`
* retrieve only the `y` and `z` columns from `df`
* retrieve only those rows where `z > 8`
* count the number of times each letter appears in these rows (`n_times`), and the sum of `z` (`z_sum`) for each letter in `y`
* arrange by descending `z_sum` values

```{r}
df %>%
  mutate(z = 4*x) %>%            # "create column" means "mutate"
  select(y, z) %>%               # "retrieve columns" means "select"
  filter(z > 8) %>%              # "retrieve rows where..."
  group_by(y) %>%                # do operations within values of a variable means "group_by"
  summarize(n_times = n(),       # aggregate operation means "summarize"
            z_sum = sum(z)) %>%
  arrange(desc(z_sum))           # "arrange by descending var" means...arrange by desc(var)
```

##### 1.3: Loading Data

We will use the following data sets as part of the workshop. The first is the [2018 US News \& World Report's Top 15 Computer Science programs](https://www.usnews.com/best-graduate-schools/top-science-schools/computer-science-rankings), constructed manually below:

```{r, message = FALSE, warning = FALSE}
top_20_cs_schools <- c("Carnegie Mellon University", 
                       "Massachusetts Institute of Technology",
                       "Stanford University",
                       "University of California, Berkeley",
                       "University of Illinois, Urbana-Champaign",
                       "Cornell University",
                       "University of Washington",
                       "Georgia Institute of Technology",
                       "Princeton University",
                       "University of Texas, Austin",
                       "California Institute of Technology",
                       "University of Michigan, Ann Arbor",
                       "Columbia University",
                       "University of California, Los Angeles",
                       "University of Wisconsin, Madison",
                       "Harvard University",
                       "University of California, San Diego",
                       "University of Maryland, College Park",
                       "University of Pennsylvania",
                       "Rice University")

unswr_rank = 1:20
csr_rank = c(1,2,4,5,3,7,6,11,20,17,65,8,12,16,13,24,9,10,14,48)
nrc_rank = c(4,2,1,5,6,7,24,12,3,14,63,17,20,10,18,9,16,15,13,47)

institution_tbl <- tibble(institution = factor(top_20_cs_schools, 
                                               levels = top_20_cs_schools), 
                          unswr_rank = unswr_rank,
                          csr_rank = csr_rank,
                          nrc_rank = nrc_rank)

institution_tbl
```

The second is a webscraped data set of manual entry graduate admissions results for students applying to 'Computer Science' programs from [The Grad Cafe](https://www.thegradcafe.com/survey/index.php), where users can submit their own data, including GPA, GRE scores, admission result and date, and comments. The challenge with this data is that *everything* is manual entry -- including the name of the school.

```{r, message = FALSE, warning = FALSE}
admissions_results <- read_csv("gradcafe_cs_results.csv")
```

Let's take a quick look at this data set. There are roughly 42,000 admissions results:

```{r}
admissions_results %>%
  skim()
```

##### 1.3.1: Exercise

Show the top schools by number of reported results. Display the top 50 results (`?print`).

```{r}
admissions_results %>%
  group_by(institution) %>%
  summarize(num_results = n()) %>%
  arrange(desc(num_results)) %>%
  print(n = 50)
```

##### 1.4: Joining Data

As enterprising graduate students, we're only interested in the admission results corresponding to USNWR's top 10 programs. Using our first table, we can perform an inner join on the the second table to get results corresponding only to those programs:

```{r}
join_results <- institution_tbl %>%
  inner_join(admissions_results, 
             by = c("institution" = "institution"))

join_results %>%
  group_by(institution) %>%
  summarize(num_results = n(),
            unswr_rank = max(unswr_rank)) %>%
  arrange(unswr_rank)
```

A naive join yields only 7 out of 10 programs with a match, and many with unbelievably few results . The other 3 programs don't have matches because the join key, `institution`, doesn't match anything in our manual entry data set. We can alleviate this problem to some extent by removing capitalizations, punctuations, and "stop words" in the join key that may result in mismatches:

```{r}
institution_tbl_adj <- institution_tbl %>%
  mutate(institution_key = str_trim(str_replace_all(str_to_lower(institution), "[:punct:]|university|institute|\\sof\\s|\\sand\\s|\\sat\\s", "")))

admissions_results_adj <- admissions_results %>%
  mutate(institution_key = str_trim(str_replace_all(str_to_lower(institution), "[:punct:]|university|institute|\\sof\\s|\\sand\\s|\\sat\\s", "")))
```

Now we try our join again:

```{r}
join_results_adj <- institution_tbl_adj %>%
  inner_join(admissions_results_adj, 
             by = c("institution_key" = "institution_key"))

join_results_adj %>%
  group_by(institution.x) %>%
  summarize(num_results = n(),
            unswr_rank = max(unswr_rank)) %>%
  arrange(unswr_rank)
```

Standardizing our join key has ensured that we find matches for every one of our top 10 programs in `institution_tbl`. But alas, just among the schools with the most results, we noticed several inconsistencies -- added abbreviations, some schools missing "University" in the name. Let's go further down the list: 

```{r}
admissions_results_adj %>%
  group_by(institution_key) %>%
  summarize(num_results = n()) %>%
  arrange(desc(num_results)) %>%
  print(n = 50)
```

We could keep applying applying new rules, e.g., remove anything in parentheses, but in general this strategy is doomed to fail as many rows will contain misspelled university names ("University of California, Berkley"), reordered components of the university names ("University of Stanford"), or department names ("Cornell Tech"). Luckily, there's an alternative strategy available -- we *could* use a fuzzy matching scheme to match where names are almost the same.

### 2: Fuzzy Matching via String Distances

The most common variant of fuzzy matching is based on the concept of string similarity or distance. If we could decide how close together two strings are in some meaningful way, we could establish a matching threshold. It's something of an art, as its efficacy depends in descending order on:

* how you decide on a string distance and match threshold
* how comfortable you are with estimated false matching rates
* whether one of your data sets has a "clean" key to match with

##### 2.1: String Distances

The first point brings us to the core principle of fuzzy matching -- string distances. A string distance is a function that accepts two strings as input, and returns a number measuring the degree of difference between the two strings. For example, if we have the strings "Northwestern", "Northeastern", and "Vanderbilt", we expect that a string distance function tells us that the first two strings are not very different, while any other pair should be identified as very different.

We'll talk about two specific examples during this workshop -- the Levenshtein distance, and Jaccard similarity -- although there are many others. 

##### 2.1.1: Levenshtein Distance

```{r}
stringdist("Northwestern", "Northwestern",
           method = "lv")

stringdist("Northwestern", c("Northwestern", "Northeastern", "Vanderbilt"),
           method = "lv")
```

##### 2.1.2: Jaccard Similarity

```{r}
# qgram distance, q = 2
stringdist("Northwestern", c("Northwestern", "Northeastern", "Vanderbilt"),
           method = "qgram", q = 2)

# jaccard similarity
stringdist("Northwestern", c("Northwestern", "Northeastern", "Vanderbilt"),
           method = "jaccard", q = 2)
```

##### 2.1.3: Exercise

Turn to your neighbors and compare your names using string distances. You can use the examples above, but you should feel free to take a look at the documentation (`?stringdist` and `?stringdist-metrics`) to see which methods and control parameters are available to you.

```{r}

```

### 2.2: Applying String Distances to Joins

Applying this concept to our data, we see many more matches in our 

```{r}
fuzzy_join_results <- institution_tbl_adj %>%
  stringdist_inner_join(admissions_results_adj, 
             by = c("institution_key" = "institution_key"),
             method = "jaccard", q = 2, max_dist = .4,
             distance_col = "dist")

fuzzy_join_results %>%
  group_by(institution.x) %>%
  summarize(num_results = n(),
            unswr_rank = max(unswr_rank)) %>%
  arrange(unswr_rank)
```

Browsing the list of matches below, we can see that only a few questionable ones arise -- "University of Washington, Seattle" matches with ~20 "University of Washington, Seattle" and "University of Washington, Tacoma" results, and we'll assume that the lone poster for "University of Washington, Milwaukee" meant to go to Wisconsin.

```{r}
fuzzy_join_results %>%
  group_by(institution.x, institution.y) %>%
  summarize(num_results = n(),
            dist = max(dist)) %>%
  arrange(desc(dist)) %>%
  print(n = 100)
```

Now, we can actually reap the fruits of our labor:

```{r}
fuzzy_join_results %>%
  mutate(gpa = ifelse(gpa <= 0 | gpa > 4, NA, gpa),
         gre_quant = ifelse(gre_quant < 130 | gre_quant > 170, NA, gre_quant),
         gre_verbal = ifelse(gre_verbal < 130 | gre_verbal > 170, NA, gre_verbal)) %>%
  filter(!is.na(gpa), degree %in% c("PhD")) %>%
  select(institution.x, 
         degree,
         gpa,
         gre_quant,
         gre_verbal) %>%
  gather(key = "metric", value="score", -institution.x, -degree) %>%
  ggplot() +
    geom_boxplot(aes(x = degree, y = score, fill = institution.x)) +
    facet_wrap(~ metric, scales = "free")
  #group_by(institution.x) %>%
  #summarize(num_results = n(),
  #          median_gpa = median(gpa, na.rm = TRUE), 
  #          median_gre_v = median(gre_verbal, na.rm = TRUE), 
  #          median_gre_q = median(gre_quant, na.rm = TRUE),
  #          unswr_rank = max(unswr_rank),
  #          pct_below_min_gpa = sum(gpa < sugg_min_gpa_reqs, na.rm = TRUE)/n()) %>%
```

```{r}
fuzzy_join_results %>%
  mutate(gpa = ifelse(gpa < 2 | gpa > 4, NA, gpa),
         gre_quant = ifelse(gre_quant < 130 | gre_quant > 170, NA, gre_quant),
         gre_verbal = ifelse(gre_verbal < 130 | gre_verbal > 170, NA, gre_verbal)) %>%
  filter(!is.na(gpa), degree == "Masters", decision %in% c("Accepted","Rejected")) %>%
  select(institution.x, 
         decision,
         gpa,
         gre_quant,
         gre_verbal) %>%
  gather(key = "metric", value="score", -institution.x, -decision) %>%
  ggplot() +
    geom_boxplot(aes(x = decision, y = score, fill = institution.x)) +
    facet_wrap(~ metric, scales = "free", ncol = 1)
  #group_by(institution.x) %>%
  #summarize(num_results = n(),
  #          median_gpa = median(gpa, na.rm = TRUE), 
  #          median_gre_v = median(gre_verbal, na.rm = TRUE), 
  #          median_gre_q = median(gre_quant, na.rm = TRUE),
  #          unswr_rank = max(unswr_rank),
  #          pct_below_min_gpa = sum(gpa < sugg_min_gpa_reqs, na.rm = TRUE)/n()) %>%
```